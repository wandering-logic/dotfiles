#ifndef INCLUDE_GUARD_CASK_H
#define INCLUDE_GUARD_CASK_H

//! \file cask.h This file contains the declarations of the cask namespace.
//! HTML documentation can be produced by running Doxygen

/// \mainpage Simple Example
///
/// The Compute Architecture Sass Kernel (CASK) API, is the internal C++
/// interface wrapping our SASS convolution and gemm kernels for NVidia teams
/// like cuDNN, TensorRT, and cuBLAS.  CASK abstracts away
/// implementation-specific details of invoking the kernels, including filling
/// in all the params we pass to the kernels, and pre-calculating book-keeping
/// details like the generated offset tables.
///
/// Invoking the CASK API is a multi-step process so the client (caller) has
/// complete control over buffer management on both the host and device.
///
/// Create descriptors for the tensors:
///
///     TensorDesc inputDesc(128, 256, 196, 196);   // params to the constructor are always N,C,H,W (even if the strides make the tensor N,H,W,C)
///     TensorDesc outputDesc(128, 10, 194, 194);
///     TensorDesc filterDesc(10, 256, 3, 3);
///
///     void* inputData = cudamalloc(inputDesc.sizeInBytes());
///     void* outputData = cudamalloc(outputDesc.sizeInBytes());
///     void* filterData = malloc(filterDesc.sizeInBytes());   // filter starts on host, not device
///
/// Fill in `inputData` and `filterData`.  It is your responsibility to get
/// `inputData` onto the device (often `inputData` will be the `outputData` of
/// the previously run kernel.)
///
///     Convolution convolution;
///
///     convolution.set...  // set reasonable inputs (alpha, beta, etc.)
///
///     convolution.setInputDesc(inputDesc);
///     convolution.setOutputDesc(outputDesc);
///     convolution.setFilterDesc(filterDesc);
///
/// Check tensor consistency
///
///     if (convolution.isConsistent() != Error::OK) error();
///
/// Find an appropriate shader by searching through availableShaders().
///
///     const ShaderList canImplement = availableShaders.findCanImplement(convolution);
///
///     const Shader* shader = 0;
///     for (ShaderList::const_iterator i = canImplement.begin();
///          i != canImplement.end();
///          i++) {
///       if (*i->has some required property) {
///         shader = *i;
///         break;
///       }
///     }
///     assert (shader && shader->canImplement(convolution));
///
///     size_t hostBufSize, deviceBufSize;
///
/// Initialize the host
///
///     if (shader->getHostReservedSize(convolution, &hostBufSize) != Error::OK) error();
///     serializableHostBuf hostBuf = malloc(hostBufSize);
///     if (shader->initHostReservedSpace(convolution, hostBufSize, hostBuf) != Error::OK) error();
///
/// Initialize the device with the filter and other reserved space data.
///
///     if (shader->getDeviceReservedSize(convolution, &deviceBufSize) != Error::OK) error();
///     serializableDeviceBuf deviceBuf = cudamalloc(deviceBufSize);
///     if (shader->initDeviceReservedSpace(convolution,
///                                         deviceBufSize, deviceBuf,
///                                         hostBufSize, hostBuf, filterData,
///                                         biasData, perChannelAlphaData, perChannelBetaData,
///                                         cudaStream) != Error::OK) error();
///
/// And actually run the convolution with the data
///
///     if (shader->run(convolution,
///                     deviceBufSize, deviceBuff, hostBufSize, hostBuf,
///                     batchSize,
///                     outputData,
///                     inputData,
///                     outputData,
///                     cudaStream) != Error::OK) error();
///
/// It is your responsibility to copy the `outputData` off the device back to
/// the host (or leave it as the input buffer for the next layer).
///
/// If the serializable host and/or device buffers have been previously saved
/// and then are deserialized into appropriate buffers, several of the steps
/// can be skipped.  We assume here that `deviceBufSize`, `hostBufSize`, and
/// `hostBuf` have all been deserialized onto the host, and that `deviceBuf`
/// has been deserialized, and copied to the device.
///
///     if (cask::getInternalVersion() != (*version number at serialization time*)) recalculateBuffers();
///     Shader* shader = availableShaders()->findByName(shaderName);
///     assert(shader && shader->canImplement(convolution));
///
///     if (shader->run(convolution,
///                     deviceBufSize, deviceBuff, hostBufSize, hostBuf,
///                     batchSize,
///                     outputData,
///                     inputData,
///                     outputData,
///                     cudaStream) != Error::OK) error();

#include <vector>
#include <string>

#if !defined(DLL_EXPORT)
#if defined(_WIN32)             // see https://sourceforge.net/p/predef/wiki/OperatingSystems/
#define DLL_EXPORT __declspec(dllexport)
#else
#define DLL_EXPORT
#endif  // _WIN32
#endif  // DLL_EXPORT

// This macro emulates enum classes for C++03.
//
// It gives scoped enum names, and disallows assignment with the wrong type,
// but doesn't protect against incorrectly comparing two enums of different
// type.  (For that we should implement all the comparison operators, and
// remove the cast operator.)
#if __cplusplus < 201103L
template<typename D>
class
DLL_EXPORT
SafeEnum : public D
{
    typedef typename D::Label Label;
public:
    SafeEnum(Label value) : mValue(value) { }
    operator Label() const { return this->mValue; }
private:
    Label mValue;
};
#define ENUMCLASS(name, ...) struct name ## _ENUMCLASS_SCOPEWRAPPER { enum Label  __VA_ARGS__ ; }; typedef SafeEnum<name ## _ENUMCLASS_SCOPEWRAPPER> name
#else
#define ENUMCLASS(name, ...) enum class name __VA_ARGS__ 
#endif


//! Contains the C++ interface to the SASS kernels
namespace cask {

///////////////////////////////////////////////////////////////////////////////////////////////////

/// \brief Current version of the library
///
/// Serialized data structures are only guaranteed to work with same version of
/// the library that created them.
///
/// \returns The current version of the library.
int
getInternalVersion();

/// Used to specify the scalar type of a tensor.
///
ENUMCLASS(ScalarType, { FP32 = 0,
            FP16,
            INT32,
            INT16,
            INT8      });

/// \brief Return type size in bytes
DLL_EXPORT
size_t
sizeInBytes(ScalarType sType);

/// This is really a stand-in for being able to test for specific ISA features.
/// The one example that doesn't fit currently is int8 instruction support.
/// Some Pascal chips have it, some not.
ENUMCLASS(GemmChip, { MAXWELL = 0, PASCAL, VOLTA });

ENUMCLASS(GemmType, { HCUDNN = 0, ICUDNN, SCUDNN,
            HCUDNN_WINOGRAD, SCUDNN_WINOGRAD,
            H884CUDNN, S884CUDNN,
            SGEMM, HGEMM, S884GEMM, H884GEMM});

ENUMCLASS(GemmShape, { RECT = 0 });

ENUMCLASS(GemmLayout, { N = 0, T });

///////////////////////////////////////////////////////////////////////////////////////////////////

/// \brief N-Tensor Descriptor (N <= 8)
///
/// Describes only the shape of the Tensor.  Pointer to actual data is passed
/// directly to Shader::run() or Shader::initDeviceReservedSpace()
///
/// Example: 32x128 Matrix (2-tensor) of fp32
/// dimensions=2, scalarType=ScalarType::FP32, dim[1]=32, dim[0]=128
///
/// element [i][j] is at element offset i*stride[1]+j*stride[0]
///
/// **row major**:
/// stride[1]=128,stride[0]=1
///
/// **column major**:
/// stride[1]=1,stride[0]=32
struct
DLL_EXPORT
TensorDesc {
    enum { MAX_DIMS=8 };
    struct Error {
        enum Type {
            OK = 0 ///, ...
        };
    };

    /// Ctor.  Will automatically fill in strides for packed NCHW layout.
    TensorDesc(size_t nSize = 0,
               size_t cSize = 0,
               size_t hSize = 0,
               size_t wSize = 0,
               ScalarType sType=ScalarType::FP32,
               size_t simd=1, //!< number of scalars per element
               int    vDim=-1) //!< which dimension is packed in elements
        ;

    /// Transform this TensorDesc into NhalfCHW2, or NChalfCHW2, or
    /// NCquarterCHW4, or whatever layout starting from a NCHW TensorDesc
    ///
    /// \returns Error::OK: on success
    /// \returns Error::???: otherwise
    Error::Type simdize(int vDim,       //!< which dimension to simdize
                        size_t simd);   //!< how many scalars per simd vector?

    size_t sizeInElements() const;
    /// size in bytes of the buffer needed to store the tensor
    inline size_t sizeInBytes() const {
        return sizeInElements() * cask::sizeInBytes(scalarType) * scalarsPerElement;
    }

    size_t     dimensions;        //!< Number of dimensions in the tensor
    ScalarType scalarType;        //!< Data type
    size_t     dim[MAX_DIMS];     //!< Number of elements in each dimension
    size_t     stride[MAX_DIMS];  //!< Distance **in elements** to the next element in this dimension.

    inline size_t getDim(size_t n) const {
        if (n < dimensions) {
            return dim[n] * ((vectorizedDim == n) ? scalarsPerElement : 1);
        }
        else {                  // error!
            return 0;
        }
    }

    size_t     scalarsPerElement; //!< Vectorization
    int        vectorizedDim;     //!< Which (if any) dimension is packed in elements  ("None" is -1)

    /// size of one element (the thing which sizes and strides are measured in)
    inline size_t elementSize() const {
        return scalarsPerElement * cask::sizeInBytes(scalarType);
    }

};

/// copy source tensor data of shape srcShape into destination tensor data of
/// shape destShape.
///
/// \pre destShape.sizeInBytes() == srcShape.sizeInBytes()
/// \pre The height and width of src and dest must be the same
/// \pre The scalarType of src and dest must be the same
/// \pre N*C*scalarsPerElement of src and dest must be the same
///
/// \post The dest buffer contains the data from src buffer in the shape
/// specified by destShape
///
/// \returns TensorDesc::Error::OK on successful data transform and copy
/// \returns TensorDesc::Error::??? otherwise
DLL_EXPORT
TensorDesc::Error
transformTensor(void* dest,   //!< destination buffer, must be destShape.sizeInBytes() large
                const void* src, //!< source buffer, must be srcShape.sizeInBytes() large
                const TensorDesc& destShape, //!< shape of the destination buffer
                const TensorDesc& srcShape   //!< shape of the source buffer
    );


///////////////////////////////////////////////////////////////////////////////////////////////////

/// \brief Describes the characteristics of the convolution operation you want
/// to run (alpha, beta, tensor shapes).  It does not choose which algorithm
/// (kernel/shader) implements the operation
class
DLL_EXPORT
Convolution {
public:
    ENUMCLASS(Error, { OK = 0,
                BATCH_SIZE,
                INPUT_DEPTH,
                OUTPUT_DEPTH,
                OUTPUT_WIDTH,
                OUTPUT_HEIGHT });

    struct Description {
        Description();
        TensorDesc inputDesc;
        TensorDesc filterDesc;
        TensorDesc outputDesc;
        ScalarType biasType;
        size_t     biasSimdWidth;
        // The padding.  (As if the stride were 1)
        size_t     convolutionPadH;
        size_t     convolutionPadW;
        // The strides
        size_t     convolutionStrideH;
        size_t     convolutionStrideW;
        // Do we apply ReLU? Add the Bias? Is it a cross-correlation? Do we use
        // the per-channel scaling?
        bool applyReLu, addBias, isCrossCorrelation, perChannelScaling;
        // The alpha/beta coefficient to compute alpha*A*B + beta*C.  Note that
        // if mPerChannelScaling is true, then the vectors are stored on the
        // device, and the pointers to them are in the ConvolutionRunConfig.
        float alpha, beta;
    };

    /// Ctor.
    Convolution();
    /// Dtor.
    virtual ~Convolution() { }

    inline const Description& getDescription() const { return mDescription; }

    /// Check tensor consistency.
    ///
    /// Checks the tensor shapes, and the padding and strides, make sense
    /// together.  Specifically: input and output N have to match.  Filter C
    /// must match Input C.  Filter K must match Output C.  Given input H
    /// (respectively W), output P (respectively Q), and filter R (respectively
    /// S), stride, and padding then we check that P =
    /// ceil((H-R+2*pad+1)/stride)
    ///
    /// \pre The setFilterDesc(), setInputDesc(), and setOutputDesc() methods
    /// have already been called.
    /// \returns Error::OK: if the tensor sizes, padding and strides are
    /// consistent.
    /// \returns Error::???: otherwise.
    virtual
    Error
    isConsistent();

    /// Set the scalar alpha parameter.
    inline void setAlpha(float alpha) { mDescription.alpha = alpha; }
    /// Set the scalar beta parameter.
    inline void setBeta(float beta) { mDescription.beta = beta; }
    inline void setBiasType(ScalarType sT) { mDescription.biasType = sT; }
    inline void setBiasSimdWidth(size_t simd=1) { mDescription.biasSimdWidth = simd; }
    // The "A" tensor is the input tensor
    inline void setADesc(const TensorDesc &inputDesc) { mDescription.inputDesc = inputDesc; }
    inline void setFilterDesc(const TensorDesc &filterDesc) { mDescription.filterDesc = filterDesc; }
    /// Set the output ("C") descriptor.
    inline void setOutputDesc(const TensorDesc &outputDesc) { mDescription.outputDesc = outputDesc; }
    /// Set whether alpha/beta are vectors of length C (as opposed to scalar,
    /// which is the default).  If alpha is a vector then it is a device buffer
    /// specified in the ConvolutionRunConfig.
    inline void setPerChannelScaling(bool perChannelScaling) {
        mDescription.perChannelScaling = perChannelScaling;
    }
    /// Set the bias flag to add the bias.
    inline void setBiasFlag(bool addBias) { mDescription.addBias = addBias; }
    /// Set the convolution vertical zero padding.
    inline void setConvolutionPadH(size_t padH) { mDescription.convolutionPadH = padH; }
    /// Set the convolution horizontal zero padding.
    inline void setConvolutionPadW(size_t padW) { mDescription.convolutionPadW = padW; }
    /// Set the convolution vertical striding.
    inline void setConvolutionStrideH(size_t strideH) { mDescription.convolutionStrideH = strideH; }
    /// Set the convolution horizontal striding.
    inline void setConvolutionStrideW(size_t strideW) { mDescription.convolutionStrideW = strideW; }
    /// Set the convolution to be a cross-correlation.
    inline void setCrossCorrelationFlag(bool flag) { mDescription.isCrossCorrelation = flag; }

    /// Set the ReLU flag to apply ReLU.
    inline void setReLuFlag(bool applyReLu) { mDescription.applyReLu = applyReLu; }

    /// Alias for setADesc
    inline void setInputDesc(const TensorDesc &inputDesc) { this->setADesc(inputDesc); }
protected:
    Description mDescription;
};

/// \brief Describes the characteristics of the wgrad (weight gradient) operation you want
/// to run (alpha, beta, tensor shapes).  It does not choose which algorithm
/// (kernel/shader) implements the operation.
///
/// The forward convolution operation is of the form \f$Z \leftarrow \alpha (A
/// * B) + \beta C + \mathrm{bias}\f$, where \f$A\f$ is the input activations,
/// \f$B\f$ is the weights, \f$C\f$ is a tensor of the same size and shape as
/// the output \f$Z\f$, and \f$\mathrm{bias}\f$ is a bias vector of the same length
/// as the number of channels in the output tensor.  The weight gradient is the
/// derivative of an error function (on the output) with respect to the weights
/// \f$B\f$.
class
DLL_EXPORT
ConvolutionWeightGradient {
public:
    ENUMCLASS(Error, { OK = 0,
                BATCH_SIZE,
                INPUT_DEPTH,
                OUTPUT_DEPTH,
                OUTPUT_WIDTH,
                OUTPUT_HEIGHT });

    struct Description {
        Description();
        size_t N, C, H, W, kWeights, kActivations;
        size_t inStrideN, inStrideC, inStrideH, inStrideW;
        ScalarType inputType;
        size_t outStrideN, outStrideC, outStrideH, outStrideW;
        ScalarType outputType;

        int32_t R, S;
        int32_t convPadH, convPadW, convStrideH, convStrideW;
        // Do we apply ReLU? Add the Bias? Is it a cross-correlation?
        bool applyReLu, addBias, isCrossCorrelation;
        float alpha, beta;
    };

    /// Ctor.
    ConvolutionWeightGradient();
    /// Dtor not needed

    inline const Description& getDescription() const { return mDescription; }

    /// Check tensor consistency.
    ///
    /// Checks the tensor shapes, and the padding and strides, make sense
    /// together.  Specifically: input and output N have to match.  Filter C
    /// must match Input C.  Filter K must match Output C.  Given input H
    /// (respectively W), output P (respectively Q), and filter R (respectively
    /// S), stride, and padding then we check that P =
    /// ceil((H-R+2*pad+1)/stride)
    ///
    /// \pre The setFilterDesc(), setInputDesc(), and setOutputDesc() methods
    /// have already been called.
    /// \returns Error::OK: if the tensor sizes, padding and strides are
    /// consistent.
    /// \returns Error::???: otherwise.
    Error
    isConsistent();

    /// Set the scalar alpha parameter.
    inline void setAlpha(float alpha) { mDescription.alpha = alpha; }
    /// Set the scalar beta parameter.
    inline void setBeta(float beta) { mDescription.beta = beta; }
    inline void setBiasType(ScalarType sT);
    inline void setBiasSimdWidth(size_t simd=1);
    // The "A" tensor is the input tensor
    inline void setADesc(const TensorDesc &inputDesc);
    inline void setFilterDesc(const TensorDesc &filterDesc);
    /// Set the output ("C") descriptor.
    inline void setOutputDesc(const TensorDesc &outputDesc);
    /// Set the bias flag to add the bias.
    inline void setBiasFlag(bool addBias) { mDescription.addBias = addBias; }
    /// Set the convolution vertical zero padding.
    inline void setConvolutionPadH(size_t padH) { mDescription.convPadH = padH; }
    /// Set the convolution horizontal zero padding.
    inline void setConvolutionPadW(size_t padW) { mDescription.convPadW = padW; }
    /// Set the convolution vertical striding.
    inline void setConvolutionStrideH(size_t strideH) { mDescription.convStrideH = strideH; }
    /// Set the convolution horizontal striding.
    inline void setConvolutionStrideW(size_t strideW) { mDescription.convStrideW = strideW; }
    /// Set the convolution to be a cross-correlation.
    inline void setCrossCorrelationFlag(bool flag) { mDescription.isCrossCorrelation = flag; }

    /// Set the ReLU flag to apply ReLU.
    inline void setReLuFlag(bool applyReLu) { mDescription.applyReLu = applyReLu; }

    /// Alias for setADesc
    inline void setInputDesc(const TensorDesc &inputDesc);
protected:
    Description mDescription;
};


///////////////////////////////////////////////////////////////////////////////////////////////////

/// \brief Describes the characteristics of the GEMM operation you want to run
/// (alpha, beta, matrix shapes).  It does not choose which algorithm
/// (kernel/shader) implements the operation
class
DLL_EXPORT
Gemm {
public:
    ENUMCLASS(Error, { OK = 0,
                MATRIX_SIZES,   // A MxK, B KxN, C MxN, either M, K, or N don't match
                MATRIX_STRIDES, // A, B, or C sizes and strides don't make sense
                MATRIX_SHAPES }); // A, B, or C batch sizes don't match

    struct Description {
        Description();
        TensorDesc inputADesc;
        TensorDesc inputBDesc;
        TensorDesc outputDesc;
        ScalarType biasType;
        size_t     biasSimdWidth;
        float      alpha;
        float      beta;
    };
    /// Ctor. Does nothing special.
    Gemm();
    /// Dtor.
    virtual ~Gemm() { }

    inline const Description& getDescription() const { return mDescription; }

    /// Set the scalar alpha parameter.
    inline void setAlpha(float alpha) { mDescription.alpha = alpha; }
    /// Set the scalar beta parameter.
    inline void setBeta(float beta) { mDescription.beta = beta; }
    // FIXME: do we need bias to be a different type than output?
    inline void setBiasType(ScalarType sT) { mDescription.biasType = sT; }
    inline void setBiasSimdWidth(size_t simd=1) { mDescription.biasSimdWidth = simd; }
    /// Set the descriptor for the first input ("A") matrix.
    inline void setADesc(const TensorDesc &inADesc) { mDescription.inputADesc = inADesc; }
    /// Set the descriptor for the second input ("B") matrix.
    inline void setBDesc(const TensorDesc &inBDesc) { mDescription.inputBDesc = inBDesc; }
    /// Set the output ("C") descriptor.
    inline void setOutputDesc(const TensorDesc &outputDesc) { mDescription.outputDesc = outputDesc; }
    
    /// Check tensor consistency.
    ///
    /// Checks the tensor shapes make sense together.
    ///
    /// \pre The setADesc(), setBDesc(), and setOutputDesc()
    /// methods have already been called.
    /// \returns Error::OK: if the tensor sizes, padding and strides are
    /// consistent.
    /// \returns Error::???: otherwise.
    virtual Error isConsistent() const;
    
protected:
    Description mDescription;

};

////////////////////////////////////////////////////////////////////////////////////////

/// \brief Struct provided by the SASS library for each kernel
///
/// The library wraps these in Shader objects of the appropriate subtype
struct
DLL_EXPORT
ShaderParams {
    const char *  name;
    // kernel is char * (very long SASS string) in CTL world
    // kernel is the procedure name associated with the CUASM code in CUBLAS
    const void *  kernel;
    // The family of the chip.
    GemmChip      chip;
    // The type of GEMM operation.
    GemmType      type;
    // The type of the inputs/output.
    ScalarType    typeA, typeB, typeC;
    // The shape of the output of GEMM.
    GemmShape     shapeC;
    // The layout of A and B.
    GemmLayout    layoutA, layoutB;
    // Elements per ldg A/B.
    unsigned      log2ElementsPerLdgA, log2ElementsPerLdgB;
    // Does it support relu and bias?
    unsigned      reLuAndBias;
    // Is it a release or experimental kernel.
    unsigned      isReleaseKernel;
    // Number of registers allocated, but there may be free holes
    unsigned      numRegisters;
    // Shared memory per CTA in bytes, ceilinged to multiple of 256B
    unsigned      sharedMemSize;
    // The output tile size.
    unsigned      elementRowsPerCta;
    unsigned      elementColsPerCta;
    // The number of threads in the CTA.
    unsigned      threadsPerCta;
    // Does kernel handle arbitrary m, n dimensions
    bool          raggedMn;
    // A single warp's stride in the k dimension, used only by CUDNN to set up delta tables.
    unsigned      warpStrideK;
    // Params for computing IncFastA, IncSlowA
    unsigned      shiftFastA;
    int           multiplierSlowA;
    int           offsetSlowA;
    // Params for computing IncFastB, IncSlowB
    unsigned      shiftFastB;
    int           multiplierSlowB;
    int           offsetSlowB;
};

/// Just typedefing void* so that it is clearer in the declarations which kind
/// of buffer is needed in each case.  (Host side or device side).
typedef void* serializableHostBuf;
typedef void* serializableDeviceBuf;

/// \brief The characterstics of an available convolution kernel.
///
/// ImplicitGemm, Winograd, etc will each subtype this.  Then there will be a
/// different instance of the struct for each variant of the kernel (small,
/// medium, large, tile sizes, etc).
struct
DLL_EXPORT
ConvolutionShader {
    ENUMCLASS(Error, { OK = 0,
                SCALAR_TYPES,       /// this shader does not support the type conversions requested
                CUDA,               /// get the specific error by calling lastCudaError() method
                BATCH_SIZE,         /// runtime batch size request doesn't fit initialized space
                CANT_PAD,           /// This shader can't handle the requested padding
                CONST_SIZE,         /// This shader doesn't have sufficient constant memory space
                LDG_MISMATCH,       /// This shader requires the "C" dimension be multiple of LDG instruction
                INTERNAL            /// An assertion in the cask implementation
                });
    const ShaderParams* shaderParams;
    const uint64_t handle;      // hash of name
    ConvolutionShader(const ShaderParams* sP = 0);
    virtual ~ConvolutionShader() { };

    virtual cudaError_t lastCudaError(const serializableHostBuf hostBuf) const = 0;

    inline bool isSmall() const {
        std::string name(this->shaderParams->name);
        return name.find("_small_") != std::string::npos;
    }
    inline bool isMedium() const {
        std::string name(this->shaderParams->name);
        return name.find("_medium_") != std::string::npos;
    }
    inline bool isLarge() const {
        std::string name(this->shaderParams->name);
        return name.find("_large_") != std::string::npos;
    }
    inline bool isSliced() const {
        std::string name(this->shaderParams->name);
        return name.find("_sliced") != std::string::npos;
    }
    inline bool isInterior() const {
        std::string name(this->shaderParams->name);
        return name.find("_interior_") != std::string::npos;
    }
    inline bool isNhwc() const {
        if (this->isNhwcAsNchw()) return false;
        std::string name(this->shaderParams->name);
        if (name.find("_nhwc_") != std::string::npos) return true;
        if (name.find("_nhwc2nchw_") != std::string::npos) return true;
        if (this->shaderParams->layoutA == GemmLayout::T) return true;
        // FIXME: I don't know if this is correct, there are _tn and _tt
        // kernels that I'm pretty sure are nhwc, but what are the
        // *winograd*_nt kernels?
        if (this->shaderParams->layoutB == GemmLayout::T) return true;
        return false;
    }
    inline bool isNhwcAsNchw() const {
        std::string name(this->shaderParams->name);
        if (name.find("nhwcAsNchw") != std::string::npos) return true;
        return false;
    }
    inline bool isNchw() const {
        return (!isNhwc());
    }
    inline bool isNhwcExp() const {
        if (isNhwc()) {
            std::string name(this->shaderParams->name);
            return name.find("_exp_") != std::string::npos;
        } else {
            return false;
        }
    }
    inline bool isStridedB() const {
        std::string name(this->shaderParams->name);
        return name.find("_stridedB_") != std::string::npos;
    }
    inline bool isSplitK() const {
        std::string name(this->shaderParams->name);
        return name.find("_splitK_") != std::string::npos;
    }
    virtual bool isDgrad() const;
    virtual bool isWgrad() const;

    inline bool isHmma884() const {
        std::string name(this->shaderParams->name);
        return name.find("884cudnn") != std::string::npos;
    }
    inline size_t getConstantMemParamOffset() const {
        switch (this->shaderParams->chip) {
            case GemmChip::MAXWELL : return 0x140;
            case GemmChip::PASCAL  : return 0x140;
            case GemmChip::VOLTA   : return 0x160;
        }
        return (size_t)-1;              // Error!
    }
    inline int  getElementsPerLdgA() const {
        return 1 << this->shaderParams->log2ElementsPerLdgA;
    }
    inline int  getElementsPerLdgB() const {
        return 1 << this->shaderParams->log2ElementsPerLdgB;
    }
    inline int  getWarpsPerCta() const {
        return this->shaderParams->threadsPerCta / 32;
    }
    inline int  getIncFastA(int strideA) const {
        return strideA << this->shaderParams->shiftFastA;
    }
    inline int  getIncFastB(int strideB) const {
        return strideB << this->shaderParams->shiftFastB;
    }
    inline ScalarType biasType() const {
        // FIXME: Fp16ToFp32Wgrad has Fp16 output, Fp32 bias/epilog???
        // 
        // generally the *bias* is the same type as the *output* with the
        // exception of Int8x4ToInt8x4:
        if (this->shaderParams->typeC == ScalarType::INT8) {
            return ScalarType::FP32;
        } else {
            return this->shaderParams->typeC;
        }
    }
    inline ScalarType epilogType() const {
        // generally the *epilog* is the same type as the *bias* with the
        // exception of Fp16ToFp16
        std::string name(this->shaderParams->name);
        
        if ((name.find("fp16_scudnn_fp16") != std::string::npos) ||
            (name.find("fp16_s884cudnn_fp16") != std::string::npos)) {
            return ScalarType::FP32;
        } else {
            return this->biasType();
        }
    }
    // NHWC-Exp (for example) needs to reuse these args for something else
    virtual int  getIncSlowA(int strideA) const;
    virtual int  getIncSlowB(int strideB) const;
    inline int  getVersion() const {
        std::string name(this->shaderParams->name);
        return ((name.find("_v1") != std::string::npos) ? 1 : 0);
    }

    int  getSlicesPerRow() const;
    int  getSlicesPerCol() const;

    inline int getKBlock() const {
        return ((isHmma884() ? 32 : 8) * getSlicesPerCol() * getSlicesPerRow());
    }

    /// Can this shader implement the requested operation?
    /// \returns Error::OK if the implementation can handle the request
    /// \returns Error::? if the implementation can't handle the request because ...
    virtual Error         canImplement(const Convolution& conv) const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by host.
    /// May be expensive operation, runs on host only.
    ///
    /// \pre Object must be in state such that isConsistent() would return true
    ///
    /// \post hostBufSize and deviceBufSize have been initialized with sizes
    /// appropriate for actually running the operation given current object
    /// state
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    getHostReservedSize(const Convolution& operation,
                        size_t*            hostBufSize) const = 0;

    /// Init host reserved space data.  May be an expensive operation.  Runs on
    /// host only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    ///
    /// \pre hostBuf is a host-side buffer of size at least hostBufSize
    ///
    /// \post hostBuf is filled in with the appropriate data for launching the
    /// operation.
    ///
    /// \post hostBuf is guaranteed to be serializable/deserializable (i.e., it
    /// contains no pointers or values that would change from run-to-run)
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initHostReservedSpace(const Convolution&  operation,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf,
                          int32_t             mode = 0) const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by device.
    /// May be expensive operation, runs on _host_ only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf has been initialized either by calling
    ///  initHostReservedSpace(), or by deserializing a buffer that was
    ///  originally created by initHostReservedSpace in the same version of
    ///  this library.  (i.e. cask::getInternalVersion() must return the same
    ///  value it returned when initHostReservedSpace() was originally called.)
    virtual
    Error
    getDeviceReservedSize(const Convolution&  operation,
                          size_t*             deviceBufSize,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf) const = 0;

    /// Init device reserved space data.  May be an expensive
    /// operation.  Runs on device.
    ///
    /// \pre deviceBufSize is the same as the value returned by getDeviceReservedSize()
    /// \pre deviceBuf is an allocated device-side buffer of size at least deviceBufSize
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf is previously initialized by initHostReservedSpace
    ///
    /// \post deviceBuf is filled in with the appropriate data for launching
    /// the operation.
    ///
    /// \post deviceBuf is guaranteed to be serializable/deserializable (i.e.,
    /// it contains no pointers or values that would change from run-to-run).
    /// Thus rather than running this routine every time, you are permitted to
    /// run it once, copy the data down to the host, serialize it, then later
    /// deserialize it, copy it to the device, and move immediately to calling
    /// run() without recalling this routine.
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initDeviceReservedSpace(const Convolution&    operation,
                            size_t                deviceBufSize,
                            serializableDeviceBuf deviceBuf,
                            size_t                hostBufSize,
                            serializableHostBuf   hostBuf,
                            const void*           hostFilterTensor, //!< KCRS 
                            const void*           hostBiasTensor, //!< optional, length K 
                            const void*           hostAlphaVector, //!< optional, length K
                            const void*           hostBetaVector, //!< optional, length K
                            cudaStream_t          cudaStream) = 0;

    /// Run the kernel on the device.
    ///
    /// \pre deviceBufSize is the same as the value returned by
    /// getDeviceReservedSize()
    ///
    /// \pre deviceBuf is allocated and initialized on the device by a previous
    /// call to initDeviceReservedSpace() in a version of the library such that
    /// cask::getInternalVersion() returns the same value.
    ///
    /// deviceInputTensor, deviceOutputTensor, and deviceCTensor are the device
    /// buffers required to perform the convolution operation.  The tensors
    /// must be *element* aligned.  The shapes of the tensors are given
    /// elsewhere (as part of the conv parameter).
    ///
    /// Operations are of the form \f$\mathrm{Output} \leftarrow \alpha
    /// (\mathrm{Input} * \mathrm{Filter}) + \beta C + \mathrm{bias}\f$.
    ///
    virtual
    Error
    run(const Convolution&    conv,
        size_t                deviceBufSize,
        serializableDeviceBuf deviceBuf,
        size_t                hostBufSize,
        serializableHostBuf   hostBuf,
        int32_t               batchSize, //!< Batch size adjustable on per-run
                                         // basis (requires batchSize <=
                                         // requested N.  batchSize <= 0
                                         // implies use batchSize = N)
        void*                 deviceOutputTensor, //!< must be size NKPQ
        const void*           deviceInputTensor, //!< must be size NCHW
        const void*           deviceCTensor, //!< May be same as OutputTensor
        cudaStream_t          cudaStream) = 0;
};

/// \brief The characterstics of an available gemm kernel.
///
/// There will be a different instance of this class for each variant of the
/// kernel (small, medium, large, tile sizes, etc).
struct
DLL_EXPORT
GemmShader {
    ENUMCLASS(Error, { OK = 0,
                SCALAR_TYPES,
                CUDA, // get the specific error by calling lastCudaError() method
                LDG_MISMATCH
                });
    const ShaderParams* shaderParams;
    const uint64_t handle;      // hash of name
    GemmShader(const ShaderParams* sP);

    virtual ~GemmShader() { };

    virtual cudaError_t lastCudaError(const serializableHostBuf hostBuf) const = 0;

//    virtual bool          isSliced();
//    virtual int           getElementsPerLdgA();
//    virtual int           getElementsPerLdgB();
//    virtual int           getWarpsPerCta();

    inline int  getIncFastA(int strideA) const {
        return strideA << this->shaderParams->shiftFastA;
    }
    inline int  getIncFastB(int strideB) const {
        return strideB << this->shaderParams->shiftFastB;
    }
    inline int  getIncSlowA(int strideA) const {
        return strideA * this->shaderParams->multiplierSlowA + this->shaderParams->offsetSlowA;
    }
    inline int  getIncSlowB(int strideB) const {
        return strideB * this->shaderParams->multiplierSlowB + this->shaderParams->offsetSlowB;
    }
    virtual int           getSlicesPerRow() const;
    virtual int           getSlicesPerCol() const;
    inline size_t getConstantMemParamOffset() const {
        switch (this->shaderParams->chip) {
            case GemmChip::MAXWELL : return 0x140;
            case GemmChip::PASCAL  : return 0x140;
            case GemmChip::VOLTA   : return 0x160;
        }
        return (size_t)-1;              // Error!
    }
    inline ScalarType epilogType() const {
        // generally the *epilog* is the same type as the *output*
        // with the exception of pseudo-fp16
        std::string name(this->shaderParams->name);
        
        if ( name.find("s884gemm_fp16") != std::string::npos ) {
            return ScalarType::FP32;
        } else {
            return this->shaderParams->typeC;
        }
    }
    inline int  getVersion() const {
        std::string name(this->shaderParams->name);
        return ((name.find("_v1") != std::string::npos) ? 1 : 0);
    }

    /// Can this shader implement the requested operation?
    /// \returns Error::OK if the implementation can handle the request
    /// \returns Error::? if the implementation can't handle the request because ...
    virtual Error         canImplement(const Gemm& conv) const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by host.
    /// May be expensive operation, runs on host only.
    ///
    /// \pre Object must be in state such that isConsistent() would return true
    ///
    /// \post hostBufSize and deviceBufSize have been initialized with sizes
    /// appropriate for actually running the operation given current object
    /// state
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    getHostReservedSize(const Gemm& operation,
                        size_t*     hostBufSize)
        const = 0;

    /// Init host reserved space data.  May be an expensive operation.  Runs on
    /// host only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    ///
    /// \pre hostBuf is a host-side buffer of size at least hostBufSize
    ///
    /// \post hostBuf is filled in with the appropriate data for launching the
    /// operation.
    ///
    /// \post hostBuf is guaranteed to be serializable/deserializable (i.e., it
    /// contains no pointers or values that would change from run-to-run)
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initHostReservedSpace(const Gemm&         operation,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf,
                          int32_t mode = 0)
        const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by device.
    /// May be expensive operation, runs on _host_ only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf has been initialized either by calling
    ///  initHostReservedSpace(), or by deserializing a buffer that was
    ///  originally created by initHostReservedSpace in the same version of
    ///  this library.  (i.e. cask::getInternalVersion() must return the same
    ///  value it returned when initHostReservedSpace() was originally called.)
    virtual
    Error
    getDeviceReservedSize(const Gemm&         operation,
                          size_t*             deviceBufSize,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf)
        const = 0;

    /// Init device reserved space data.  May be an expensive
    /// operation.  Runs on device.
    ///
    /// \pre deviceBufSize is the same as the value returned by getDeviceReservedSize()
    /// \pre deviceBuf is an allocated device-side buffer of size at least deviceBufSize
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf is previously initialized by initHostReservedSpace
    ///
    /// \post deviceBuf is filled in with the appropriate data for launching
    /// the operation.
    ///
    /// \post deviceBuf is guaranteed to be serializable/deserializable (i.e.,
    /// it contains no pointers or values that would change from run-to-run).
    /// Thus rather than running this routine every time, you are permitted to
    /// run it once, copy the data down to the host, serialize it, then later
    /// deserialize it, copy it to the device, and move immediately to calling
    /// run() without recalling this routine.
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initDeviceReservedSpace(const Gemm&           operation,
                            size_t                deviceBufSize,
                            serializableDeviceBuf deviceBuf,
                            size_t                hostBufSize,
                            serializableHostBuf   hostBuf,
                            cudaStream_t          cudaStream)
        = 0;

    /// Run the kernel on the device.
    ///
    /// \pre deviceBufSize is the same as the value returned by getDeviceReservedSize()
    ///
    /// \pre deviceBuf is allocated and initialized on the device by a previous
    /// call to initDeviceReservedSpace() in a version of the library such that
    /// cask::getInternalVersion() returns the same value.
    /// \brief Runn the kernel on the device.
    ///
    /// A, B, C, and Output are the device buffers required to perform the
    /// matrix multiply.
    ///
    /// The matrices must be *element* aligned.  The shapes of the tensors will
    /// be given elsewhere (to the Gemm class).
    ///
    /// Operations are of the form \f$Z \leftarrow \alpha (A \times B) + \beta
    /// C\f$.
    virtual
    Error
    run( const Gemm&           gemm,
         size_t                deviceBufSize,
         serializableDeviceBuf deviceBuf,
         size_t                hostBufSize,
         serializableHostBuf   hostBuf,
         void*                 deviceOutput,
         const void*           deviceA,
         const void*           deviceB,
         cudaStream_t          cudaStream) = 0;
};

/// \brief The characterstics of an available convolution weight gradient kernel.
struct
DLL_EXPORT
WeightGradientShader {
    ENUMCLASS(Error, { OK = 0,
                SCALAR_TYPES,       /// this shader does not support the type conversions requested
                CUDA,               /// get the specific error by calling lastCudaError() method
                BATCH_SIZE,         /// runtime batch size request doesn't fit initialized space
                CANT_PAD,           /// This shader can't handle the requested padding
                CONST_SIZE,         /// This shader doesn't have sufficient constant memory space
                LDG_MISMATCH,       /// This shader requires the "C" dimension be multiple of LDG instruction
                INTERNAL            /// An assertion in the cask implementation
                });
    const ShaderParams* shaderParams;
    const uint64_t handle;      // hash of name
    WeightGradientShader(const ShaderParams* sP = 0);
    virtual ~WeightGradientShader() { };

    virtual cudaError_t lastCudaError(const serializableHostBuf hostBuf) const = 0;

    inline bool isSmall() const {
        std::string name(this->shaderParams->name);
        return name.find("_small_") != std::string::npos;
    }
    inline bool isMedium() const {
        std::string name(this->shaderParams->name);
        return name.find("_medium_") != std::string::npos;
    }
    inline bool isLarge() const {
        std::string name(this->shaderParams->name);
        return name.find("_large_") != std::string::npos;
    }
    inline bool isSliced() const {
        std::string name(this->shaderParams->name);
        return name.find("_sliced") != std::string::npos;
    }
    inline bool isInterior() const {
        std::string name(this->shaderParams->name);
        return name.find("_interior_") != std::string::npos;
    }
    inline bool isNhwc() const {
        if (this->isNhwcAsNchw()) return false;
        std::string name(this->shaderParams->name);
        if (name.find("_nhwc_") != std::string::npos) return true;
        if (name.find("_nhwc2nchw_") != std::string::npos) return true;
        if (this->shaderParams->layoutA == GemmLayout::T) return true;
        if (this->shaderParams->layoutB == GemmLayout::T) return true;
        return false;
    }
    inline bool isNhwcAsNchw() const {
        std::string name(this->shaderParams->name);
        if (name.find("nhwcAsNchw") != std::string::npos) return true;
        return false;
    }
    inline bool isNchw() const {
        return (!isNhwc());
    }
    inline bool isNhwcExp() const {
        if (isNhwc()) {
            std::string name(this->shaderParams->name);
            return name.find("_exp_") != std::string::npos;
        } else {
            return false;
        }
    }
    inline bool isStridedB() const {
        std::string name(this->shaderParams->name);
        return name.find("_stridedB_") != std::string::npos;
    }
    inline bool isSplitK() const {
        std::string name(this->shaderParams->name);
        return name.find("_splitK_") != std::string::npos;
    }
    inline bool isDgrad() const {
        std::string name(this->shaderParams->name);
        return name.find("_dgrad_") != std::string::npos;
    }
    inline bool isWgrad() const {
        std::string name(this->shaderParams->name);
        return name.find("_wgrad_") != std::string::npos;
    }
    inline bool isHmma884() const {
        std::string name(this->shaderParams->name);
        return name.find("884cudnn") != std::string::npos;
    }
    inline size_t getConstantMemParamOffset() const {
        switch (this->shaderParams->chip) {
            case GemmChip::MAXWELL : return 0x140;
            case GemmChip::PASCAL  : return 0x140;
            case GemmChip::VOLTA   : return 0x160;
        }
        return (size_t)-1;              // Error!
    }
    inline int  getElementsPerLdgA() const {
        return 1 << this->shaderParams->log2ElementsPerLdgA;
    }
    inline int  getElementsPerLdgB() const {
        return 1 << this->shaderParams->log2ElementsPerLdgB;
    }
    inline int  getWarpsPerCta() const {
        return this->shaderParams->threadsPerCta / 32;
    }
    inline int  getIncFastA(int strideA) const {
        return strideA << this->shaderParams->shiftFastA;
    }
    inline int  getIncFastB(int strideB) const {
        return strideB << this->shaderParams->shiftFastB;
    }
    inline ScalarType biasType() const {
        // FIXME: Fp16ToFp32Wgrad has Fp16 output, Fp32 bias/epilog???
        // 
        // generally the *bias* is the same type as the *output* with the
        // exception of Int8x4ToInt8x4:
        if (this->shaderParams->typeC == ScalarType::INT8) {
            return ScalarType::FP32;
        } else {
            return this->shaderParams->typeC;
        }
    }
    inline ScalarType epilogType() const {
        // generally the *epilog* is the same type as the *bias* with the
        // exception of Fp16ToFp16
        std::string name(this->shaderParams->name);
        
        if ((name.find("fp16_scudnn_fp16") != std::string::npos) ||
            (name.find("fp16_s884cudnn_fp16") != std::string::npos)) {
            return ScalarType::FP32;
        } else {
            return this->biasType();
        }
    }
    // NHWC-Exp (for example) needs to reuse these args for something else
    virtual int  getIncSlowA(int strideA) const;
    virtual int  getIncSlowB(int strideB) const;
    inline int  getVersion() const {
        std::string name(this->shaderParams->name);
        return ((name.find("_v1") != std::string::npos) ? 1 : 0);
    }

    int  getSlicesPerRow() const;
    int  getSlicesPerCol() const;

    inline int getKBlock() const {
        return ((isHmma884() ? 32 : 8) * getSlicesPerCol() * getSlicesPerRow());
    }

    /// Can this shader implement the requested operation?
    /// \returns Error::OK if the implementation can handle the request
    /// \returns Error::? if the implementation can't handle the request because ...
    virtual Error         canImplement(const ConvolutionWeightGradient& conv) const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by host.
    /// May be expensive operation, runs on host only.
    ///
    /// \pre Object must be in state such that isConsistent() would return true
    ///
    /// \post hostBufSize and deviceBufSize have been initialized with sizes
    /// appropriate for actually running the operation given current object
    /// state
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    getHostReservedSize(const ConvolutionWeightGradient& operation,
                        size_t*            hostBufSize) const = 0;

    /// Init host reserved space data.  May be an expensive operation.  Runs on
    /// host only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    ///
    /// \pre hostBuf is a host-side buffer of size at least hostBufSize
    ///
    /// \post hostBuf is filled in with the appropriate data for launching the
    /// operation.
    ///
    /// \post hostBuf is guaranteed to be serializable/deserializable (i.e., it
    /// contains no pointers or values that would change from run-to-run)
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initHostReservedSpace(const ConvolutionWeightGradient&  operation,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf,
                          int32_t             mode = 0) const = 0;

    /// Calculate the reserved space buffer sizes (in bytes) needed by device.
    /// May be expensive operation, runs on _host_ only.
    ///
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf has been initialized either by calling
    ///  initHostReservedSpace(), or by deserializing a buffer that was
    ///  originally created by initHostReservedSpace in the same version of
    ///  this library.  (i.e. cask::getInternalVersion() must return the same
    ///  value it returned when initHostReservedSpace() was originally called.)
    virtual
    Error
    getDeviceReservedSize(size_t*             deviceBufSize,
                          size_t              hostBufSize,
                          serializableHostBuf hostBuf) const = 0;

    /// Init device reserved space data.  May be an expensive
    /// operation.  Runs on device.
    ///
    /// \pre deviceBufSize is the same as the value returned by getDeviceReservedSize()
    /// \pre deviceBuf is an allocated device-side buffer of size at least deviceBufSize
    /// \pre hostBufSize is the same as the value returned by getHostReservedSize()
    /// \pre hostBuf is previously initialized by initHostReservedSpace
    ///
    /// \post deviceBuf is filled in with the appropriate data for launching
    /// the operation.
    ///
    /// \post deviceBuf is guaranteed to be serializable/deserializable (i.e.,
    /// it contains no pointers or values that would change from run-to-run).
    /// Thus rather than running this routine every time, you are permitted to
    /// run it once, copy the data down to the host, serialize it, then later
    /// deserialize it, copy it to the device, and move immediately to calling
    /// run() without recalling this routine.
    ///
    /// \returns Error::OK on success.
    /// \returns Error::??? otherwise.
    virtual
    Error
    initDeviceReservedSpace(size_t                deviceBufSize,
                            serializableDeviceBuf deviceBuf,
                            size_t                hostBufSize,
                            serializableHostBuf   hostBuf,
                            const void*           hostFilterTensor, //!< KCRS 
                            const void*           hostBiasTensor, //!< optional, length K 
                            cudaStream_t          cudaStream) = 0;

    /// Run the kernel on the device.
    ///
    /// \pre deviceBufSize is the same as the value returned by
    /// getDeviceReservedSize()
    ///
    /// \pre deviceBuf is allocated and initialized on the device by a previous
    /// call to initDeviceReservedSpace() in a version of the library such that
    /// cask::getInternalVersion() returns the same value.
    ///
    /// deviceInputTensor, deviceOutputTensor, and deviceCTensor are the device
    /// buffers required to perform the convolution operation.  The tensors
    /// must be *element* aligned.  The shapes of the tensors are given
    /// elsewhere (as part of the conv parameter).
    ///
    /// Operations are of the form \f$\mathrm{Output} \leftarrow \alpha
    /// (\mathrm{Input} * \mathrm{Filter}) + \beta C + \mathrm{bias}\f$.
    ///
    virtual
    Error
    run(size_t                deviceBufSize,
        serializableDeviceBuf deviceBuf,
        size_t                hostBufSize,
        serializableHostBuf   hostBuf,
        void*                 deviceOutputTensor, //!< must be size NKPQ
        const void*           deviceInputTensor, //!< must be size NCHW
        const void*           deviceCTensor, //!< May be same as OutputTensor
        cudaStream_t          cudaStream) = 0;
};

/// the architecture of GPU0 on the host
GemmChip hostArchitecture();

template<typename ShaderType, typename OperationType>
class
DLL_EXPORT
ShaderList {
    typedef std::vector<ShaderType*> ImplType;
public:
    /// ctor
    ShaderList() : impl(), isSorted(false) { }

    /// dtor
    ~ShaderList() { }

    // a forward iterator (input iterator that can be reused)
    typedef typename ImplType::const_iterator const_iterator;

    inline const_iterator begin() const { sortHandles(); return impl.begin(); }
    inline const_iterator end() const { sortHandles(); return impl.end(); }
    ShaderType* findByName(const std::string& name) const;
    ShaderType* findByHandle(uint64_t handle) const;
    ShaderList findCanImplement(const OperationType& operation,
                                GemmChip forChip=hostArchitecture()) const;

    inline void push_back(ShaderType* shader) {
        isSorted = false;
        impl.push_back(shader);
    }
private:
    bool isSorted;
    ImplType impl;
    // sortHandles modifies the order of the impl vector, but does not change
    // its semantics, so it is semantically const.  (Implementation uses
    // const_cast())
    inline void sortHandles() const;
};

typedef ShaderList<ConvolutionShader,Convolution> ConvShaderList;
typedef ShaderList<GemmShader,Gemm> GemmShaderList;

/// this is the list of all available shaders.
const ConvShaderList* availableConvShaders();
const GemmShaderList* availableGemmShaders();

// hacky side channel for turning on debugging messages
extern int isPrintParams;
// hacky side channel for turning off kernel launches while debugging
extern int suppressGPULaunch;

} // namespace cask

#endif // INCLUDE_GUARD_CASK_H
